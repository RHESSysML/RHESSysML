---
title: "Variable Importance Workflow, Sagehen - Gradient Boosting"
author: "Alex Clippinger, Wylie Hampson, Shale Hunter, Peter Menzies"
date: '2022-06-03'
output: 
  rmarkdown::html_document:
    toc: true
    toc_float: true
    code_folding: show
    theme: cerulean
---

# Introduction to Gradient Boosting

This workflow is meant as a compliment to the random forest workflow, which is available in the `notebooks` folder. While random forest is a "quick and easy" machine learning technique that has bee used to determine variable importance for ecohydrological models like RHESSys before, it is far from the only method that can be used for this purpose. While the scope of this project does not allow a full exploration of all the possibilities of machine learning applications to RHESSys output, gradient boosting provides a good comparison to random forest because of a few important conceptual similarities and differences. 

Firstly, both random forest and gradient boosting are tree-based machine learning models, which means the mechanisms by which the models make decisions are similar, at the most basic level - this allows for a direct comparison of model performance that is easy to understand from a human perspective. But at the same time, gradient boosting has a number of different hyperparameters that allow for greater user control over the model than can be achieved using random forest. In a predictive model, this is generally favorable because it leads to greater predictive power; however, gradient boosting is less commonly used to identify variable importance, which is the purpose of these workflows.

As you will see, the extra time and computational demands of creating a gradient boosting model may not be an efficient tradeoff compared to a random forest model which may be nearly as accurate. However, it may still prove useful as an external validation of importance measures generated using random forest, or as an avenue to begin exploring further applications of machine learning to RHESSys data.

# Setup

## renv:

To help ensure reproducibility, the packages and package versions used to build this workflow have been saved via [`renv`](https://rstudio.github.io/renv/articles/renv.html). To download any missing packages and load the correct versions, run the command `renv::restore()` in the console. Doing so will not impact your package versions outside of this R Project. It may take several minutes depending on the number of discrepancies.

```{r setup, include = TRUE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(scipen = 999)

# Standard packages
library(tidyverse)
library(here)
library(patchwork)
library(psych)
library(kableExtra)
library(zeallot)
library(DT)

# Machine Learning packages
library(caret)
library(party)
library(partykit)
library(permimp)
library(gbm)
library(spatialRF)
```


# Data Description

Note that this workflow will use data that is aggregated by water year but **not** separated by climate scenario - to find the workflow that uses gradient boosting on climate-separated data, find the appropriate document in the `notebooks` folder.

## Load Data

```{r}
load(here::here("data", "input", "prepared_data_sagehen.RData"))
```

First, we want to get a summary description of the data set. This is crucial for many reasons:

1.  Shows from the start if any variables do not have the expected range, magnitude, class, etc.

2.  Provides information on data set characteristics that are important for decisions later on in the machine learning process.

Here, we display summary statistics for the base climate scenario data.

```{r describe_data}
source(here::here("R", "summarize_data.R"))

summarize_data(df_clim0)
summarize_data(df_clim2)
```

We can also look at the key study area characteristics. In this Sagehen Creek study area, there are six landscape positions where RHESSys was run.

```{r describe_strata}
strata <- df %>%
  filter(clim == 0) %>% 
  select(c(stratumID, topo, elev, aspect, slope)) %>%
  group_by(stratumID, topo) %>% 
  summarize_if(is.numeric, mean) %>% 
  mutate(topo = case_when(topo == "M" ~ "Mid-Slope",
                          topo == "U" ~ "Upslope",
                          topo == "R" ~ "Riparian")) %>%
  dplyr::arrange(topo) %>%
  rename("Stratum" = stratumID,
         "Topo" = topo,
         "Elevation (m)" = elev,
         "Aspect (degrees CCW from East)" = aspect,
         "Slope (degrees)" = slope)

DT::datatable(strata,
              caption = "Summary of 6 landscape positions in the Sagehen Creek study area")
```

# Remove Multicollinearity

Highly correlated predictor variables are not as much a concern in machine learning when creating a predictive model. However, for this process of assessing relative predictor variable importance, multicollinear variables have biased importance (Strobl et al. 2008). Therefore, these need to be handled prior to assessing feature importance.

## Method

Below, we use Variance Inflation Factors (VIF) and Pearson Correlation Coefficients to remove variables with high multicollinearity. This is implemented using the `auto_vif()` and `auto_cor()` functions from the `spatialRF` package. Both functions allow the user to define an order of preference for the selection of variables, which will be discussed below. If no preference order is decided, the `auto_vif()` function orders the variables from minimum to maximum VIF, and the `auto_cor()` function orders variables by their column order.

The `auto_vif()` function works by initially starting with the variable with highest preference. Then, it iterates through the preference order list, computing the VIF for each new variable added. If the VIF of the new variable is lower than the threshold it is kept, if not the variable is removed. This process is continued until all VIF values are below the user-input threshold.

Similarly, the `auto_cor()` function works by computing a correlation matrix for all variables. Next, it computes the maximum correlation for each variable. The function then begins with the variable with lowest preference. If that variables maximum correlation coefficient is above the user-input threshold, that variable is removed. This process is continued until all correlation values are below the user-input threshold.

## Identify and Remove Correlated Variables

First, we create data frames containing only the predictor variables to assist with the next steps.

```{r get_predictors}
# Save data frames of predictor variables for first climate scenario
df_clim0_num_preds <- df_clim0 %>% 
  select(!response & where(is.numeric))

df_clim0_fpreds <- df_clim0 %>% 
  select(!response & where(is.factor)) %>% 
  colnames()

# Save data frames of predictor variables for second climate scenario
df_clim2_num_preds <- df_clim2 %>% 
  select(!response & where(is.numeric))

df_clim2_fpreds <- df_clim2 %>% 
  select(!response & where(is.factor)) %>% 
  colnames()
```

Below, there are two methods for setting preference order: (1) Manually creating an ordered vector of column names, or (2) Allowing a preliminary random forest method to determine preference order based on variable importance. Note that in the preliminary random forest, highly correlated variables will not produce accurate estimates of importance. However, we assume that relative importance is reasonably accurate to support the selection between highly correlated variables. The second method is used by default. 

1. Run this chunk to manually create an ordered vector of column names. The vector does not need to include the names of all predictors - only those that you would like to keep in the analysis.

```{r manual_preference_order}
# Preference order can be determined manually for variables of interest:

#pref_order0 <- c("precip", "rz_storage", "trans", "evap")
#pref_order2 <- c("precip", "rz_storage", "trans", "evap")
```

2. Run this chunk to allow preliminary random forest to automatically determine preference order. Note: even though we will be using Gradient Boosting her for the actual analysis, preliminary importance is set using a Random Forest because it is faster.

```{r auto_preference_order, warning=FALSE}
# First climate scenario

# Find preliminary importance using random forest
prelim_imp_clim0 <- train(x = df_clim0 %>% select(!response),
              y = df_clim0$response,
              method = 'rf',
              importance = TRUE,
              replace = TRUE,
              trControl = trainControl(method = "none", seed = 4326))

# Set preference order based on variable importance
pref_order0 <- varImp(prelim_imp_clim0$finalModel, scale = FALSE) %>% 
  arrange(-Overall) %>% 
  rownames()

# Second climate scenario

# Find preliminary importance using random forest
prelim_imp_clim2 <- train(x = df_clim2 %>% select(!response),
              y = df_clim2$response,
              method = 'rf',
              importance = TRUE,
              replace = TRUE,
              trControl = trainControl(method = "none", seed = 4326))

# Set preference order based on variable importance
pref_order2 <- varImp(prelim_imp_clim2$finalModel, scale = FALSE) %>% 
  arrange(-Overall) %>% 
  rownames()
```

Thresholds for VIF and correlation can be set below, with default values of 5 and 0.75, respectively. Increasing the thresholds will reduce the number of variables that get removed, but it will increase the likelihood that collinearity influences the results.

```{r set_thresholds}
vif.threshold = 5
cor.threshold = 0.75
```

```{r remove_multicollinearity, warning = FALSE}
source(here::here("R", "remove_vif.R"))
source(here::here("R", "remove_cor.R"))

# Create list of selected variables
clim0_vif <- remove_vif(df_clim0_num_preds, vif.threshold = vif.threshold, pref_order0)$selected.variables
clim0_cor <- remove_cor(df_clim0_num_preds, cor.threshold = cor.threshold, pref_order0)$selected.variables
clim0_select_variables <- intersect(clim0_vif, clim0_cor)

clim2_vif <- remove_vif(df_clim2_num_preds, vif.threshold = vif.threshold, pref_order2)$selected.variables
clim2_cor <- remove_cor(df_clim2_num_preds, cor.threshold = cor.threshold, pref_order2)$selected.variables
clim2_select_variables <- intersect(clim2_vif, clim2_cor)

# Combine selected variables with factors
df_clim0_preds <- c(all_of(df_clim0_fpreds), all_of(clim0_select_variables))
df_clim2_preds <- c(all_of(df_clim2_fpreds), all_of(clim2_select_variables))

# Remove numeric variables with multicollinearity
df_clim0_reduced <- df_clim0 %>% 
  select(c(response, all_of(df_clim0_preds)))

df_clim2_reduced <- df_clim2 %>% 
  select(c(response, all_of(df_clim2_preds)))
```

## Summary of Removed Variables

The next step is intended to elucidate the previous multicollinearity reduction process by creating tables and plots showing which variables were selected and removed, and why. This information can be used to determine whether the auto-generated preference order based on preliminary importance performed satisfactorily, or whether the preference order should be set manually.

```{r summarize_removed_vars_function, class.source='fold-hide'}
source(here::here("R", "summarize_var_removal.R"))
source(here::here("R", "plot_removed_cor.R"))
```

The following functions output two tables and two plots of all variables indicating selection status and the criteria by which those selections were made. The first table shows preliminary importance and VIF, and the second shows correlation between each variable combination. Preliminary importance and VIF are also plotted visually in the two bar charts. 

### Climate Scenario 0 {.tabset}

#### Variable Removal Summary Table

```{r vif_summary_table_0, warning=FALSE}
summarize_var_removal(df_clim0_num_preds, clim = 0, table = TRUE) 
```

#### Correlation Summary Plot

```{r cor_summary_table_0}
plot_removed_cor(clim0_select_variables, df_clim0_num_preds)
```


### Climate Scenario 2 {.tabset}

#### Variable Romoval Summary Table

```{r vif_summary_table_2, warning=FALSE}
plot_removed_cor(clim0_select_variables, df_clim2_num_preds)
```

#### Correlation Summary Plot

```{r cor_summary_table_2}
summarize_var_removal(df_clim2_num_preds, clim = 2, table = TRUE)
```


# Feature Importance with Gradient Boosting

## Gradient Boosting-Specific Data Preparation: One-Hot Encoding

Most gradient boosting packages like the one used in this workflow are unable to directly process categorical variables (there are exceptions, such as `LightGBM` - but for the sake of consistency and compatibility with `caret`, we won't explore that package here). This means that in order for the gradient boosting model to be able to handle categorical variables such as `stratumID`, `clim`, `scen`, and `topo` they must be one-hot encoded. One-hot encoding is essentially a means to convert categorical variables to numerical variables by splitting each unique level of the categorical variable into its own column; then, for each observation where the original categorical variable takes on that level the new variable is given a value of 1. All observations that are at different levels of the original variable are given a value of 0.

Additionally, the `gbm` package requires that the response variable is split out from the predictor variables to train the model.

```{r}
# Predictor vs response variables, climate scenario 0
y0 = df_clim0$response
x0 = df_clim0[-1]

# Predictor vs response variables, climate scenario 2
y2 = df_clim2$response
x2 = df_clim2[-1]

# One-hot encoding, climate scenario 0
x0[,df_clim0_fpreds] <- lapply(x0[,df_clim0_fpreds], factor)
xhot0 = x0 %>% data.table::data.table() %>% mltools::one_hot(cols = df_clim0_fpreds)

# One-hot encoding, climate scenario 2
x2[,df_clim2_fpreds] <- lapply(x2[,df_clim2_fpreds], factor)
xhot2 = x2 %>% data.table::data.table() %>% mltools::one_hot(cols = df_clim2_fpreds)
```


## Hyper-Parameter Tuning

There is a bit more complexity that goes into tuning a Gradient Boosting model than a Random Forest. The `expand.grid()` function will allow us to test multiple values of each parameter to optimize model performance. Some of the standard hyperparameters that we will tune here are:

 - `n.trees` (Number of Trees)
 - `interaction_depth` (Maximum Nodes Per Tree)
 - `shrinkage` (Shrinkage/Learning Rate)
 - `n.minobsinnode` (Minimum Observations in Terminal Nodes)
 
All these parameters are used to prevent overfitting, which is a more common problem in boosting models than random forests. The inherent randomness of random forests prevents overfitting relatively effectively, but more care is required with gradient boosting. For example, the `colsample_bytree` parameter mimics the functionality of a random forest model by randomly sampling the specified proportions of columns to build a boosted tree.

Additionally, we will use `trainControl()` to implement 10-fold cross-validation to further avoid overfitting.

```{r}

fitControl <- caret::trainControl(
  method = "cv",
  number = 10,
  savePredictions = 'final')

tune_grid <- expand.grid(
  n.trees = seq(from = 200, to = 800, by = 50),
  shrinkage = c(0.01, 0.02, 0.05, 0.1, 0.3),
  interaction.depth = c(2, 3, 4, 5, 6), 
  n.minobsinnode = c(10, 30, 50))

```

Note: these parameters are unique to the `gbm` package; there are many packages to implement gradient boosting 

Running the code chunk below essentially trains a new gradient boosting model for every possible combination of the parameters listed above in the `tune_grid` object; this means that 1,200 individual models are trained! This is slow.

*To save time when running this workflow, the code to generate the gradient boosting model is commented out, and a pre-run model is loaded from the `models` folder. If you would like to experiment with creating your own model, uncomment the code below - just be aware that it will take a while! Alternatively, small manual tweaks can be made to the model by looking at the `bestTune` object inside of `gbm_model_cv`, then using a modified `tune_grid` that only has single numbers instead of vectors for each hyper-parameter.*

```{r}
set.seed(4326)

# gbm_model_cv0 <- caret::train(
#   x = xhot0,
#   y = y0,
#   trControl = fitControl,
#   tuneGrid = tune_grid,
#   metric = 'RMSE',
#   method = "gbm",
#   verbose = FALSE)

gbm_model_cv0 <- read_rds(here::here("data/input/GBm0.RDS"))
# saveRDS(gbm_model_cv0, file = here::here("data", "input", "GBm0.RDS"))
```

```{r}
set.seed(4326)

# gbm_model_cv2 <- caret::train(
#   x = xhot2,
#   y = y2,
#   trControl = fitControl,
#   tuneGrid = tune_grid,
#   metric = 'RMSE',
#   method = "gbm",
#   verbose = FALSE)

gbm_model_cv2 <- read_rds(here::here("data/input/GBm2.RDS"))
# saveRDS(gbm_model_cv2, file = here::here("data", "input", "GBm2.RDS"))
```


To get the model with the best-performing parameter set from each cliamte scenario from the tuning grid above, we simply select the `bestTune` element from our models. Running these two models individually will then allow us to extract accuracy and importance metrics more easily in the following sections.

```{r}
best_grid0 <- gbm_model_cv0$bestTune

model_clim0 <- caret::train(
  x = xhot0,
  y = y0,
  trControl = fitControl,
  tuneGrid = best_grid0,
  metric = 'RMSE',
  method = "gbm",
  verbose = FALSE)

best_grid2 <- gbm_model_cv2$bestTune

model_clim2 <- caret::train(
  x = xhot2,
  y = y2,
  trControl = fitControl,
  tuneGrid = best_grid2,
  metric = 'RMSE',
  method = "gbm",
  verbose = FALSE)
```


### Assessing Feature Importance

Assessing feature importance is a complex task with many possible approaches. Tree based models like random forest and gradient boosting offer convenient "split-improvement" measures, like mean increase in purity and minimum depth, which are intrinsically derived during model construction. However, these have been shown to be biased towards variables with many categories or large value ranges (Strobl et al. 2007). Despite some of their shortcomings, these importance measures can provide insights that are further explored in the random forest workflow.

As the primary measure of importance we instead use partial permutation importance via the `varImp` function. Permutation importance has been tested and determined to be the most unbiased importance metric for data with a mix of categorical and continuous variables with a variety of classes and ranges, respectively. This is calculated in the following steps:

1. Assess prediction accuracy (mean squared error) of the model on the out-of-bag data.

2. Permute the values of a given variable.

3. Feed the dataset with the permuted values into the Random Forest and reassess model accuracy.

4. Importance of the permuted variable is deemed to be the mean loss in accuracy when its values were permuted.

```{r}

gbm_imp0 = varImp(model_clim0, scale = FALSE)
imp_clim0 = gbm_imp0$importance
imp_clim0 = imp_clim0 %>% mutate(Variable = row.names(imp_clim0)) %>% 
  filter(imp_clim0$Overall != 0) %>% 
  mutate(Rank = rank(-Overall)) %>% 
  arrange(Rank)

gbm_imp2 = varImp(model_clim2, scale = FALSE)
imp_clim2 = gbm_imp2$importance
imp_clim2 = imp_clim2 %>% mutate(Variable = row.names(imp_clim2)) %>% 
  filter(imp_clim2$Overall != 0) %>% 
  mutate(Rank = rank(-Overall)) %>% 
  arrange(Rank)

```

# Model Evaluation

The following section provides visualizations and statistics to evaluate the gradient boosting model performance.

```{r}
gb_fit0 <- round(tail(model_clim0$results$Rsquared, 1)*100, 2)

gb_fit2 <- round(tail(model_clim2$results$Rsquared, 1)*100, 2)

```

The model explained `r gb_fit2`% of variance in NPP in the 2 degree climate warming scenario, and `r gb_fit0`% of variance in NPP in the current climate scenario.

# Visualize results

```{r}
source(here::here("R", "df_imp_table.R"))

df_imp_table(imp_clim0, imp_clim2)
```

```{r plot_imp}
source(here::here("R", "plot_imp.R"))

clim0_plot <- plot_imp(imp_clim0)
clim2_plot <- plot_imp(imp_clim2)

clim0_plot + clim2_plot
```


```{r}
model_clim0$results$RMSE
model_clim0$results$Rsquared

model_clim2$results$RMSE
model_clim2$results$Rsquared
```


# Further Exploration

## Shiny App

The capstone team created an application with a suite of interactive visualization tools using the `Shiny` package. It can be used to further explore the original data set and the variable importance results from the workflow. The app can be launched below by setting `run_app` below to `TRUE`.

First, the following objects are saved for use in the app.

```{r save_model_output}
save(model_clim0, model_clim2, imp_clim0, imp_clim2,  
     file = here::here("data", "output", "model_output_sagehen.RData"))
```

Run the following code chunk to launch the Shiny app (note you will have to set `run_app` to TRUE):

```{r}
run_app <- FALSE

if (run_app == TRUE) {
  shiny::runApp(here::here("shiny_sagehen"))
}
```
